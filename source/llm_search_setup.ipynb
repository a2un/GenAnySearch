{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "export HUGGING_FACE_HUB_TOKEN=\"<secret token>\"\n",
    "vllm serve 'meta-llama/Meta-Llama-3.1-405B-Instruct' --device=cpu\n",
    "\n",
    "# Deploy with docker on Linux:\n",
    "docker run --runtime nvidia --gpus all \\\n",
    "\t--name vllm_phi3_steelhacks \\\n",
    "\t-v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    " \t--env \"HUGGING_FACE_HUB_TOKEN=<secret token>\" \\\n",
    "\t-p 8000:8000 \\\n",
    "\t--ipc=host \\\n",
    "\tvllm/vllm-openai:latest \\\n",
    "\t--model microsoft/Phi-3-mini-4k-instruct\n",
    "\n",
    "# Load and run the model:\n",
    "docker exec -it vllm_phi3_steelhacks bash -c \"vllm serve microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# Call the server using curl:\n",
    "curl -X POST \"http://localhost:8000/v1/chat/completions\" \\ \n",
    "\t-H \"Content-Type: application/json\" \\ \n",
    "\t--data '{\n",
    "\t\t\"model\": \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\t\t\"messages\": [\n",
    "\t\t\t{\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "\t\t]\n",
    "\t}'\n",
    "\n",
    "docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama \n",
    "docker exec -it ollama ollama run llama3.1:8b\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(fname,user_query):\n",
    "    with open(f'../prompts/{fname}.txt','r') as pf1:\n",
    "        return pf1.read().format(**{\n",
    "            'user-query':user_query\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LLM_response(llm_model,fname,user_query):\n",
    "\n",
    "    prompt = make_prompt(fname,user_query)\n",
    "    \n",
    "    msg_to_llm = [\n",
    "        {\n",
    "            \"role\":\"system\",\n",
    "            \"content\":\"Your role is rewrite natural language queries into parameter maps to use in API calls.\"\n",
    "        }\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=llm_model,#\"codellama\",\n",
    "        # model = \"mistral\",\n",
    "        messages=msg_to_llm,\n",
    "        stream=False,\n",
    "        options={\n",
    "            \"temperature\":0,\n",
    "            # \"repeat_penalty\":1.5,\n",
    "            # \"max_tokens\":1024\n",
    "            # \"top_p\":1,\n",
    "            # \"frequency_penalty\":0,\n",
    "            # \"presence_penalty\":0\n",
    "        }\n",
    "        \n",
    "    ) \n",
    "    \n",
    "    ollama.generate(model=llm_model,prompt=prompt)\n",
    "    \n",
    "    return response['message']['content']\n",
    "    # return response['response']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided a user query in free text form search. \n",
      "Convert this into a list of parameters and their corresponding values in JSON format where indicated as \"user parameters\". \n",
      "After generating these parameters, then generate a second JSON\n",
      "with descriptions to these parameters where indicated as \"parameter descriptions\". \n",
      "Be brief and concise.\n",
      "Generate only where indicated. \n",
      "DO NOT generate any free text ONLY JSON.\n",
      "\n",
      "test\n",
      "\n",
      "user parameters:\n",
      "parameter descriptions:\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_prompt('prompt1',\"test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requested free text user query:\n",
      "\n",
      "I know JAVA but I don't know python, I am not sure what course to take next which covers python in sufficient depth but \n",
      "also covers other things I could do as a programmer I have an A in most courses\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query = \"\"\"\n",
    "I know JAVA but I don't know python, I am not sure what course to take next which covers python in sufficient depth but \n",
    "also covers other things I could do as a programmer I have an A in most courses\n",
    "\n",
    "\"\"\"\n",
    "# user_query = ' '.join(str(input()).split())\n",
    "print(f\"Requested free text user query:\\n{user_query}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_LLM_response('llama3:8b','prompt1',user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the JSON outputs:\n",
      "\n",
      "**User Parameters**\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"name\": \"query\",\n",
      "    \"value\": \"I know JAVA but I don't know python, I am not sure what course to take next which covers python in sufficient depth but also covers other things I could do as a programmer I have an A in most courses\"\n",
      "  }\n",
      "]\n",
      "```\n",
      "\n",
      "**Parameter Descriptions**\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"name\": \"query\",\n",
      "    \"description\": \"The user's search query in free text form.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
